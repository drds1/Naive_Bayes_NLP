{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Natural Language Processing\n",
    "\n",
    "This article will perform sentiment analysis on a series of tweets. Some of which pertain to natural disaster / terrorist incidents (class 1), the others are innocent drivel (class 0). This post will review the Naive Bayes approach to classifying text. \n",
    "\n",
    "\n",
    "##  How to calculate Naive Bayes Probabilities\n",
    "\n",
    "https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c\n",
    "We want to find the probability of a certain class 1 or 2 given some input text X. In maths speak this is P(C_j | X).  We can calculate this from Bayes theorem as \n",
    "\n",
    "P(C_j | X) = P(X | C_j) P(C_j) / P(X).\n",
    "\n",
    "From the above, we have three quantities to calculate.\n",
    "\n",
    "#### __P(X)__: \n",
    "\n",
    "Give that all probabilities have P(X) as the denominator, this constant can be disregarded.\n",
    "\n",
    "#### __P(C_j):__ \n",
    "\n",
    "This is simple enough. It is just the relative fraction of class _i_ in the data set (i.e. for the positive class, this is the number of positives divided by the total number of samples in the training data.\n",
    "\n",
    "#### __P(X | C_j):__\n",
    "\n",
    "Representing the input as a set of features x1, x2 ...xn, P(X) = P(x1, x2...xn). We can rewrite P(X | C_j) as P(x1,x2...xn | C_j). We use the 'Bag of Words' assumption to calculate this quantity. This assumes the order of the words doesnt matter (an incorrect assumption but one that often doesnt introduce too much error). Naive Bayes assumes features (or words) are independent so\n",
    "\n",
    "P(x1,x2...xn | C_j) = P(x1|C_j) P(x2 | C_j) .... P(xn | C_j).\n",
    "\n",
    "The probability of word 1 given class j, P(x1 | C_j), is then\n",
    "\n",
    "P(xi | C_j) = count (xi, C_j) / sum_k (xk, C_j).\n",
    "\n",
    "i.e. this counts all occurrences of word xi in all inputs of class C_j and divides them by the sum of counts of all words in the vocabulary. Laplace smoothing can be used to mitigate the effects of zero occurrences of words and dividing by zero.\n",
    "\n",
    "In summary, the Naive Bayes probability for input X, P(X | C_j) P(C_j), is calculated by multiplying together P(xi | C_j) for all words in our vocabulary and multiplying the result by P(C_J). Lets now set one up using sci-kit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn implementation:\n",
    "\n",
    "Python's scitkit-learn module includes excellent Naive Bayes functionality for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
