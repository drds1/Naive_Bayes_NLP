{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Natural Language Processing\n",
    "\n",
    "This article will review a bread-and-butter natural language processing (NLP) model known as the Naive Bayes Classifier. We will analyse a series of tweets with binary outcomes from the [kaggle disaster dataset](https://www.kaggle.com/c/nlp-getting-started). These tweets show peoples' online reaction to natural disasters world wide. The goal is to build a model to automatically classify which tweets pertain to disasters and which do not.\n",
    "\n",
    "In this post we will learn\n",
    "\n",
    "1): What is a Naive Bayes classifier?\n",
    "\n",
    "2): Python scikit-learn implementation\n",
    "\n",
    "3): How do we measure model performance?\n",
    "\n",
    "##  What is a Naive Bayes Classifier?\n",
    "\n",
    "Training a model to label a sample of text as positive / negative\n",
    "is known as Natural Language Processing (NLP) binary classification.\n",
    "\n",
    "A common choice of model for such a task is the Naive Bayes classifier.\n",
    "Using [Bayes theorem](https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c\n",
    "),\n",
    "we can formulate our outcome variable as the probability of a certain class 0 or 1 given some input text X. In our case, a tweet referring to a recent natural disaster is labeled 1 and a neutral tweet 0. In maths speak this is known as the Naive Bayes probability P(C_j | X), which is calculated from Bayes theorem as\n",
    "\n",
    "P(C_j | X) = P(X | C_j) P(C_j) / P(X).\n",
    "\n",
    "From the above, we have three quantities to calculate.\n",
    "\n",
    "#### __P(X)__: \n",
    "\n",
    "Give that all probabilities have P(X) as the denominator, this constant can be disregarded.\n",
    "\n",
    "#### __P(C_j):__ \n",
    "\n",
    "This is simple enough. It is just the relative\n",
    "fraction of class _i_ in the data set (i.e.\n",
    "for the positive class, this is the number of\n",
    "positives divided by the total number of samples\n",
    "in the training data).\n",
    "\n",
    "#### __P(X | C_j):__\n",
    "\n",
    "Representing the input as a set of features\n",
    "x1, x2 ...xn, P(X) = P(x1, x2...xn).\n",
    "We can rewrite P(X | C_j) as P(x1,x2...xn | C_j).\n",
    "Now comes the trick. The word _Naive_ in Naive Bayes\n",
    "classifiers means that we make the assumption that\n",
    "probabilities of all words are independent of\n",
    "each other. This chiefly means that we assume\n",
    "the order of the words doesn't matter (an incorrect assumption\n",
    "but one that often doesn't introduce too much error), but\n",
    " also allows us to rewrite our conditional\n",
    " probability as\n",
    "\n",
    "P(x1,x2...xn | C_j) = P(x1|C_j) P(x2 | C_j) .... P(xn | C_j).\n",
    "\n",
    "The probability of word 1 given class j, P(x1 | C_j), is then\n",
    "\n",
    "P(xi | C_j) = count (xi, C_j) / sum_k (xk, C_j).\n",
    "\n",
    "i.e. this counts all occurrences of word xi in\n",
    "all inputs of class C_j and divides them by the\n",
    "sum of counts of all words in the vocabulary.\n",
    "Laplace smoothing can be used to mitigate the\n",
    "effects of zero occurrences of words and\n",
    "dividing by zero.\n",
    "\n",
    "In summary, the Naive Bayes probability for\n",
    "input X, P(X | C_j) P(C_j), is calculated by\n",
    "multiplying together P(xi | C_j) for all words\n",
    "in our vocabulary and multiplying the result by\n",
    "P(C_J). Lets now set one up using sci-kit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Scikit-learn Implementation:\n",
    "\n",
    "Python's scitkit-learn module includes excellent Naive Bayes functionality for building and NLP classifier. To begin, lets load our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "#for data ingestion manipulation\n",
    "import pandas as pd\n",
    "\n",
    "#to split data into train/test samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#to generate our sparse matrix of word occurrence counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#to normalise over tweet length and word overuse\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#the Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#combine word count, transformation, fit / predict steps into a single pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# to compute the ROC curve and Area AUC\n",
    "from sklearn import metrics\n",
    "\n",
    "# to visualise the ROC\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('./nlp_data/train.csv')\n",
    "X, y = list(df['text']), list(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the word counts for the train data\n",
    "\n",
    "We now must construct the probabilities _P(X|C_j)_ for each word / class combination. We therefore need the occurrence counts of each word in each of our input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the word counts for the train data\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count transformations\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer tweets will have higher \n",
    "average count values than shorter tweets, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of \n",
    "occurrences of each word in a tweet by the total number of words in \n",
    "the tweet: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur \n",
    "in many tweets in the sample and are therefore less informative than those \n",
    "that occur only in a smaller portion of the tweets (e.g words like \"the\", \"and\" etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict the labels for the test data\n",
    "\n",
    "First we get the word counts and apply the transformations as to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline\n",
    "\n",
    "The count, transformation and predict steps can be combined into one using scikit-learns pipeline functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we Measure Model Performance?\n",
    "\n",
    "To measure the performance of our classifier, we could simply use an accuracy metric from our true positive (TP), false positive (FP), true negative (TN) and false negative (FN) counts. This is defined as\n",
    "\n",
    "`accuracy = TP + FP / (TP + FP + TN + FN)`\n",
    "\n",
    "This is a common performance metric for a classifier but is suceptible to class imballances. When testing for a disease for example, most of the training sample will likely not be infected. In these cases a classifier that always predicted negative would achieve a high accuracy score despite being useless. It is better therefore to use a metric that combines the following concepts:\n",
    "\n",
    "#### Precision = TP / (TP + FP)\n",
    "Is the fraction of positive classifications that were correct.\n",
    "\n",
    "#### Recall = TP / (TP + FN) \n",
    "Is the fraction of positive that were identified by the classifier.\n",
    "\n",
    "\n",
    "Remember that classifiers output the probability of an input belonging to each class. We then choose an arbitrary threshold probability to say whether the probability indicates one class or another. A good way of measuring the performance of a classifier is to construct the [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve trailing various threshold probabilities, and measuring the area under the curve (AUC). Fortunately, scikit-learn comes to the rescue again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print('ROC curve AUC = '+str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising ROC curve\n",
    "\n",
    "We can plot the roc curve as true positive rate vs false positive rate using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "idx = np.argsort(fpr)\n",
    "ax1.plot(fpr[idx],tpr[idx],label='Naive Bayes Classifier')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Here we have quickly reviewed the stats behind the Naive Bayes text classification algorithm, constructed a Naive Bayes classifier in Python and written code to measure its performance.\n",
    "\n",
    "Naive Bayes models are one of the simpler text classification models and often outperformed by neural net type methods (long short term memory networks are a good choice for text classification). Despite this, simpler models mean less potential for the model to overfit, a smaller training sample, and the simpler model interpret-ability. Opting for a simpler classifier model over a neural net can save lots of pain further down the line and will often get you then answer you need, and perform almost as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
